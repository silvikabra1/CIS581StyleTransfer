{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Config\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "from getpass import getpass\n",
    "import os\n",
    "import openai\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter OpenAI API key:\n"
     ]
    }
   ],
   "source": [
    "print('Enter OpenAI API key:')\n",
    "openai.api_key = getpass()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter comma-separated style descriptors to transfer onto your video. If you want a gap between styles, enter None. Ex: \"ponitlism, None, cubism, leaf\"\n",
      "Andy Warhol, modern art, None, mosaic\n"
     ]
    }
   ],
   "source": [
    "# Instructions for user input\n",
    "print('Enter comma-separated style descriptors to transfer onto your video. If you want a gap between styles, enter None. Ex: \"ponitlism, None, cubism, leaf\"')\n",
    "prompt = input()\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransfer:\n",
    "\n",
    "    MAX_CHANNEL_INTENSITY = 255.0\n",
    "    \n",
    "    def __init__(self, config=Config):\n",
    "        self.config = config\n",
    "        self.hub_module = hub.load(self.config.TENSORFLOW_HUB_HANDLE)\n",
    "        self.pre_frame_dir = glob.glob(f'{self.config.PRE_VID_FRAME_DIR}/*')\n",
    "        self.post_frame_dir = glob.glob(f'{self.config.POST_VID_FRAME_DIR}/*')\n",
    "        self.style_dir = glob.glob(f'{self.config.STYLE_DIR}/*')\n",
    "        delete_these_files = self.style_dir\n",
    "        if self.config.CLEAR_INPUT_FRAME_CACHE:\n",
    "            delete_these_files += self.post_frame_dir\n",
    "            delete_these_files += self.pre_frame_dir\n",
    "        \n",
    "        for file in delete_these_files:\n",
    "            os.remove(file)\n",
    "\n",
    "         # Update contents of directory after deletion\n",
    "        self.pre_frame_dir = glob.glob(f'{self.config.PRE_VID_FRAME_DIR}/*')\n",
    "        self.post_frame_dir = glob.glob(f'{self.config.POST_VID_FRAME_DIR}/*')\n",
    "\n",
    "        if len(self.pre_frame_dir) > 0:\n",
    "            self.frame_width = cv2.imread(self.pre_frame_dir[0]).shape[1]\n",
    "\n",
    "    def fetch_pre_frames(self):\n",
    "        if len(self.pre_frame_dir) > 0:\n",
    "            print(\"Using cached frames\")\n",
    "            return\n",
    "        \n",
    "        video = cv2.VideoCapture(self.config.PRE_VID_PATH)\n",
    "        frame_interval = np.floor((1.0 / self.config.FPS) * 1000)\n",
    "        ret, frame = video.read()\n",
    "\n",
    "        if frame is None:\n",
    "            raise ValueError(f\"Error: No video provided\")\n",
    "    \n",
    "        ## Adjust scale based on specified frame height\n",
    "        scale = self.config.FRAME_HEIGHT / frame.shape[0]\n",
    "        self.frame_width = int(frame.shape[1] * scale)\n",
    "        \n",
    "        frame = cv2.resize(frame, (self.frame_width, self.config.FRAME_HEIGHT)).astype(np.uint8)\n",
    "        cv2.imwrite(self.config.PRE_VID_FRAME_PATH.format(0), frame)\n",
    "        \n",
    "        ## Sample original video at specified frame rate\n",
    "        offset = 1\n",
    "        while ret:\n",
    "            timestamp = offset * frame_interval\n",
    "            video.set(cv2.CAP_PROP_POS_MSEC, timestamp)\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.resize(frame, (self.frame_width, self.config.FRAME_HEIGHT)).astype(np.uint8)\n",
    "            cv2.imwrite(self.config.PRE_VID_FRAME_PATH.format(offset), frame)\n",
    "            offset += 1\n",
    "        \n",
    "        self.pre_frame_dir = glob.glob(f'{self.config.PRE_VID_FRAME_DIR}/*')\n",
    "\n",
    "    def generate_styles(self):\n",
    "        # Parse prompt and get image URLs\n",
    "        style_inputs = prompt.split(\", \")\n",
    "        style_inputs = [None if input == \"None\" else input for input in style_inputs]\n",
    "        self.style_inputs = style_inputs\n",
    "        for idx, input in enumerate(style_inputs):\n",
    "            if input is None:\n",
    "                continue\n",
    "            else:\n",
    "                response = openai.Image.create(\n",
    "                    prompt=input,\n",
    "                    n=1,\n",
    "                    size=\"1024x1024\"\n",
    "                )\n",
    "                image_url = response['data'][0]['url']\n",
    "                response = requests.get(image_url).content\n",
    "                file_name = self.config.STYLE_IMG_PATH.format(idx)\n",
    "                with open(file_name, 'wb') as handler:\n",
    "                    handler.write(response)\n",
    "\n",
    "    # input to Jimmy's part: \"None, pointilism, None, cubism\" - [None, filename, None, filename]\n",
    "    def fetch_style_refs(self):\n",
    "        num_frames = len(self.pre_frame_dir)\n",
    "        style_ref_imgs = list()\n",
    "        style_ref_img_resized = False\n",
    "        style_ref_img_files = sorted(self.style_dir)\n",
    "        self.num_ref_imgs = len(self.style_inputs) \n",
    "        self.num_frames_per_style = num_frames if self.num_ref_imgs == 1 else np.ceil(num_frames / (self.num_ref_imgs - 1))\n",
    "\n",
    "        # Make all style ref imgs same size as first style\n",
    "        style_ref_img_1_height = None\n",
    "        style_ref_img_1_width = None\n",
    "\n",
    "        curr_style_img_index = 0\n",
    "        for i in range(self.num_ref_imgs):\n",
    "            # Check index of style inputs to see if it is None\n",
    "            if self.style_inputs[i] is None:\n",
    "                style_ref_imgs.append(None)\n",
    "                continue\n",
    "            style_ref_img = cv2.imread(style_ref_img_files[curr_style_img_index])\n",
    "            style_ref_img = cv2.cvtColor(style_ref_img, cv2.COLOR_BGR2RGB)\n",
    "            if style_ref_img_1_height is None or style_ref_img_1_width is None:\n",
    "                style_ref_img_1_height, style_ref_img_1_width, channels = style_ref_img.shape\n",
    "            else:\n",
    "                style_ref_img_height, style_ref_img_width, channels = style_ref_img.shape\n",
    "                # Change these style imgs to match first style img\n",
    "                if style_ref_img_1_height != style_ref_img_height or style_ref_img_1_width != style_ref_img_width:\n",
    "                    style_ref_img = cv2.resize(style_ref_img, (style_ref_img_1_width, style_ref_img_1_height))\n",
    "                    style_ref_img_resized = True\n",
    "            style_ref_imgs.append(style_ref_img / self.MAX_CHANNEL_INTENSITY)\n",
    "            curr_style_img_index += 1\n",
    "            \n",
    "        self.style_ref_imgs = style_ref_imgs\n",
    "        # Alert user that style images were resized\n",
    "        if style_ref_img_resized:\n",
    "            print(\"Warning: Resizing style images -> may cause distortion\")\n",
    "\n",
    "    def trim_img(self, img):\n",
    "        return img[:self.config.FRAME_HEIGHT, :self.frame_width]\n",
    "\n",
    "    def fetch_post_frames(self):\n",
    "        self.pre_frame_dir = glob.glob(f'{self.config.PRE_VID_FRAME_DIR}/*')\n",
    "        ghost_frame = None\n",
    "        for frame_idx, filename in enumerate(sorted(self.pre_frame_dir)):\n",
    "            if frame_idx % 10 == 0:\n",
    "                print(f\"Output frame: {(frame_idx/len(self.pre_frame_dir)):.0%}\")\n",
    "            current_frame = cv2.imread(filename) \n",
    "            current_frame = cv2.cvtColor(current_frame, cv2.COLOR_BGR2RGB) / self.MAX_CHANNEL_INTENSITY\n",
    "            curr_style_img_idx = int(frame_idx / self.num_frames_per_style)\n",
    "            blend_ratio = 1 - ((frame_idx % self.num_frames_per_style) / self.num_frames_per_style)\n",
    "            inv_blend_ratio = 1 - blend_ratio\n",
    "\n",
    "            prev_style = self.style_ref_imgs[curr_style_img_idx]\n",
    "            next_style = self.style_ref_imgs[curr_style_img_idx + 1]\n",
    "        \n",
    "            # If both are content images, don't need to apply style transfer - TEST out\n",
    "            if prev_style is None and next_style is None:\n",
    "                temp_ghost_frame = cv2.cvtColor(ghost_frame, cv2.COLOR_RGB2BGR) * self.MAX_CHANNEL_INTENSITY\n",
    "                cv2.imwrite(self.config.POST_VID_FRAME_PATH.format(frame_idx), temp_ghost_frame)\n",
    "                continue\n",
    "            \n",
    "            if frame_idx > 0:\n",
    "                current_frame = ((1 - self.config.GHOST_FRAME_TRANSPARENCY) * current_frame) + (self.config.GHOST_FRAME_TRANSPARENCY * ghost_frame)\n",
    "            current_frame = tf.cast(tf.convert_to_tensor(current_frame), tf.float32)\n",
    "\n",
    "            # if previous img is not a style img, then next must be a style img, which will be what we blend\n",
    "            if prev_style is None:\n",
    "                img_to_blend = next_style\n",
    "            # if next img is not a style img, then prev must be a style img, which will be what we blend\n",
    "            elif next_style is None:\n",
    "                img_to_blend = prev_style\n",
    "            else:\n",
    "                blended_prev_style = blend_ratio * prev_style\n",
    "                blended_next_style = inv_blend_ratio * next_style\n",
    "                img_to_blend = blended_prev_style + blended_next_style\n",
    "\n",
    "            img_to_blend = tf.cast(tf.convert_to_tensor(img_to_blend), tf.float32)\n",
    "            expanded_blended_img = tf.constant(tf.expand_dims(img_to_blend, axis=0))\n",
    "            expanded_current_frame = tf.constant(tf.expand_dims(current_frame, axis=0))\n",
    "            \n",
    "            # Apply style transfer\n",
    "            stylized_frame = self.hub_module(expanded_current_frame, expanded_blended_img).pop()\n",
    "            stylized_frame = tf.squeeze(stylized_frame)\n",
    "\n",
    "            # Re-blend if one of the images is a content image\n",
    "            if prev_style is None:\n",
    "                reblended_prev_style = blend_ratio * current_frame\n",
    "                reblended_next_style = inv_blend_ratio * stylized_frame\n",
    "            if next_style is None:\n",
    "                reblended_prev_style = blend_ratio * stylized_frame\n",
    "                reblended_next_style = inv_blend_ratio * current_frame\n",
    "            if prev_style is None or next_style is None:\n",
    "                stylized_frame = self.trim_img(reblended_prev_style) + self.trim_img(reblended_next_style)\n",
    "\n",
    "            if self.config.KEEP_COLORS:\n",
    "                stylized_frame = self.color_correct_to_input(current_frame, stylized_frame)\n",
    "            \n",
    "            ghost_frame = np.asarray(self.trim_img(stylized_frame))\n",
    "\n",
    "            temp_ghost_frame = cv2.cvtColor(ghost_frame, cv2.COLOR_RGB2BGR) * self.MAX_CHANNEL_INTENSITY\n",
    "            cv2.imwrite(self.config.POST_VID_FRAME_PATH.format(frame_idx), temp_ghost_frame)\n",
    "        self.post_frame_dir = glob.glob(f'{self.config.POST_VID_FRAME_DIR}/*')\n",
    "\n",
    "    def color_correct_to_input(self, input, precorrected_output):\n",
    "        # Change image so that it is compatible with OpenCV\n",
    "        input = np.array((input * self.MAX_CHANNEL_INTENSITY), dtype=np.float32)\n",
    "        input = cv2.cvtColor(input, cv2.COLOR_BGR2YCR_CB)\n",
    "        pre_corrected_output = np.array((pre_corrected_output * self.MAX_CHANNEL_INTENSITY), dtype=np.float32)\n",
    "        pre_corrected_output = cv2.cvtColor(pre_corrected_output, cv2.COLOR_BGR2YCR_CB)\n",
    "        pre_corrected_output = self.trim_img(pre_corrected_output)\n",
    "        # Extract the channels, merge the intensities and color spaces\n",
    "        color_corrected_output = np.zeros(pre_corrected_output.shape, dtype=np.float32)\n",
    "        color_corrected_output[:, :, 0] = pre_corrected_output[:, :, 0]\n",
    "        color_corrected_output[:, :, 1] = pre_corrected_output[:, :, 1]\n",
    "        color_corrected_output[:, :, 2] = pre_corrected_output[:, :, 2]\n",
    "        return cv2.cvtColor(color_corrected_output, cv2.COLOR_YCrCb2BGR) / self.MAX_CHANNEL_INTENSITY\n",
    "\n",
    "\n",
    "    def generate_stylized_video(self):\n",
    "        self.post_frame_dir = glob.glob(f'{self.config.POST_VID_FRAME_DIR}/*')\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        # print(\"frame width\", self.frame_width)\n",
    "        video_writer = cv2.VideoWriter(self.config.POST_VID_PATH, fourcc, self.config.FPS, (self.frame_width, self.config.FRAME_HEIGHT))\n",
    "\n",
    "        # Write frames to video\n",
    "        for count, filename in enumerate(sorted(self.post_frame_dir)):\n",
    "            if count % 10 == 0:\n",
    "                print(f\"Saving frame: {(count/len(self.post_frame_dir)):.0%}\")\n",
    "            image = cv2.imread(filename)\n",
    "            video_writer.write(image)\n",
    "\n",
    "        video_writer.release()\n",
    "        print(f\"Style transfer complete! Output at {self.config.POST_VID_PATH}\")\n",
    "    \n",
    "    def run(self):\n",
    "        print(\"Generating style references\")\n",
    "        self.generate_styles()\n",
    "        print(\"Fetching input frames\")\n",
    "        self.fetch_pre_frames()\n",
    "        print(\"Fetching style reference info\")\n",
    "        self.fetch_style_refs()\n",
    "        print(\"Fetching output frames\")\n",
    "        self.fetch_post_frames()\n",
    "        print(\"Saving video\")\n",
    "        self.generate_stylized_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating style references\n",
      "Fetching input frames\n",
      "Using cached frames\n",
      "Fetching style reference info\n",
      "Fetching output frames\n",
      "Output frame: 0%\n",
      "Output frame: 6%\n",
      "Output frame: 12%\n",
      "Output frame: 18%\n",
      "Output frame: 25%\n",
      "Output frame: 31%\n",
      "Output frame: 37%\n",
      "Output frame: 43%\n",
      "Output frame: 49%\n",
      "Output frame: 55%\n",
      "Output frame: 61%\n",
      "Output frame: 67%\n",
      "Output frame: 74%\n",
      "Output frame: 80%\n",
      "Output frame: 86%\n",
      "Output frame: 92%\n",
      "Output frame: 98%\n",
      "Saving video\n",
      "Saving frame: 0%\n",
      "Saving frame: 6%\n",
      "Saving frame: 12%\n",
      "Saving frame: 18%\n",
      "Saving frame: 25%\n",
      "Saving frame: 31%\n",
      "Saving frame: 37%\n",
      "Saving frame: 43%\n",
      "Saving frame: 49%\n",
      "Saving frame: 55%\n",
      "Saving frame: 61%\n",
      "Saving frame: 67%\n",
      "Saving frame: 74%\n",
      "Saving frame: 80%\n",
      "Saving frame: 86%\n",
      "Saving frame: 92%\n",
      "Saving frame: 98%\n",
      "Style transfer complete! Output at ./post_video.mp4\n"
     ]
    }
   ],
   "source": [
    "## Run as a script\n",
    "StyleTransfer().run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
