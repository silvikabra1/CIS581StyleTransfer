{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Config\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransfer:\n",
    "\n",
    "    MAX_CHANNEL_INTENSITY = 255.0\n",
    "    \n",
    "    def __init__(self, config=Config):\n",
    "        self.config = config\n",
    "        self.hub_module = hub.load(self.config.TENSORFLOW_HUB_HANDLE)\n",
    "        self.pre_frame_dir = glob.glob(f'{self.config.PRE_VID_FRAME_DIR}/*')\n",
    "        self.post_frame_dir = glob.glob(f'{self.config.POST_VID_FRAME_DIR}/*')\n",
    "        self.style_dir = glob.glob(f'{self.config.STYLE_DIR}/*')\n",
    "        self.ref_img_count = len(self.config.STYLE_SEQ)\n",
    "        delete_these_files = list()\n",
    "\n",
    "        if self.config.CLEAR_INPUT_FRAME_CACHE:\n",
    "            delete_these_files += self.post_frame_dir\n",
    "            delete_these_files += self.pre_frame_dir\n",
    "        \n",
    "        for file in delete_these_files:\n",
    "            os.remove(file)\n",
    "\n",
    "         # Update contents of directory after deletion\n",
    "        self.pre_frame_dir = glob.glob(f'{self.config.PRE_VID_FRAME_DIR}/*')\n",
    "        self.post_frame_dir = glob.glob(f'{self.config.POST_VID_FRAME_DIR}/*')\n",
    "\n",
    "        if len(self.pre_frame_dir) > 0:\n",
    "            self.frame_width = cv2.imread(self.pre_frame_dir[0].shape[1])\n",
    "\n",
    "    def fetch_pre_frames(self):\n",
    "        if len(self.pre_frame_dir) > 0:\n",
    "            print(\"Using cached frames\")\n",
    "            return\n",
    "        \n",
    "        video = cv2.VideoCapture(self.config.PRE_VID_PATH)\n",
    "        frame_interval = np.floor((1.0 / self.config.FPS) * 1000)\n",
    "        ret, frame = video.read()\n",
    "\n",
    "        if frame is None:\n",
    "            raise ValueError(f\"Error: No video provided\")\n",
    "    \n",
    "        ## Adjust scale based on specified frame height\n",
    "        scale = self.config.FRAME_HEIGHT / frame.shape[0]\n",
    "        self.frame_width = int(frame.shape[1] * scale)\n",
    "        \n",
    "        frame = cv2.resize(frame, (self.frame_width, self.config.FRAME_HEIGHT)).astype(np.uint8)\n",
    "        cv2.imwrite(self.config.PRE_VID_FRAME_PATH.format(0), frame)\n",
    "        \n",
    "        ## Sample original video at specified frame rate\n",
    "        offset = 1\n",
    "        while ret:\n",
    "            timestamp = offset * frame_interval\n",
    "            video.set(cv2.CAP_PROP_POS_MSEC, timestamp)\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.resize(frame, (self.frame_width, self.config.FRAME_HEIGHT)).astype(np.uint8)\n",
    "            cv2.imwrite(self.config.PRE_VID_FRAME_PATH.format(offset), frame)\n",
    "            offset += 1\n",
    "        \n",
    "        self.pre_frame_dir = glob.glob(f'{self.config.PRE_VID_FRAME_DIR}/*')\n",
    "\n",
    "\n",
    "    def fetch_style_refs(self):\n",
    "        frame_len = len(self.pre_frame_dir)\n",
    "        style_ref_imgs = list()\n",
    "        style_ref_img_resized = False\n",
    "        style_ref_img_files = sorted(self.style_dir)\n",
    "        # update t const if poss\n",
    "        self.t_const = frame_len if self.ref_img_count == 1 else np.ceil(frame_len / (self.ref_img_count - 1))\n",
    "        self.transition_style_seq_list = list()\n",
    "\n",
    "        # Make all style ref imgs same size as first style\n",
    "        style_ref_img_1_height = None\n",
    "        style_ref_img_1_width = None\n",
    "\n",
    "        for style_ref_img_file in style_ref_img_files:\n",
    "            style_ref_img = cv2.imread(style_ref_img_file)\n",
    "            style_ref_img = cv2.cvtColor(style_ref_img, cv2.COLOR_BGR2RGB)\n",
    "            if style_ref_img_1_height is None or style_ref_img_1_width is None:\n",
    "                style_ref_img_1_height, style_ref_img_1_width, channels = style_ref_img.shape\n",
    "            else:\n",
    "                style_ref_img_height, style_ref_img_width, channels = style_ref_img.shape\n",
    "                # Change these style imgs to match first style img\n",
    "                if style_ref_img_1_height != style_ref_img_height or style_ref_img_1_width != style_ref_img_width:\n",
    "                    style_ref_img = cv2.resize(style_ref_img, (style_ref_img_1_width, style_ref_img_1_height))\n",
    "                    style_ref_img_resized = True\n",
    "            style_ref_imgs.append(style_ref_img / self.MAX_CHANNEL_INTENSITY)\n",
    "            \n",
    "        # Alert user that style images were resized\n",
    "        if style_ref_img_resized:\n",
    "            print(\"Warning: Resizing style images -> may cause distortion\")\n",
    "        \n",
    "        for i in range(self.ref_img_count):\n",
    "            style_seq_num = self.config.STYLE_SEQ[i]\n",
    "            if style_seq_num is None:\n",
    "                self.transition_style_seq_list.append(None)\n",
    "            else:\n",
    "                self.transition_style_seq_list.append(style_ref_imgs[style_seq_num])\n",
    "\n",
    "    def trim_img(self, img):\n",
    "        return img[:self.config.FRAME_HEIGHT, :self.frame_width]\n",
    "\n",
    "    def fetch_post_frames(self):\n",
    "        self.pre_frame_dir = glob.glob(f'{self.config.PRE_VID_FRAME_DIR}/*')\n",
    "        ghost_frame = None\n",
    "        for count, filename in enumerate(sorted(self.pre_frame_dir)):\n",
    "            if count % 10 == 0:\n",
    "                print(f\"Output frame: {(count/len(self.pre_frame_dir)):.0%}\")\n",
    "            content_img = cv2.imread(filename) \n",
    "            content_img = cv2.cvtColor(content_img, cv2.COLOR_BGR2RGB) / self.MAX_CHANNEL_INTENSITY\n",
    "            curr_style_img_index = int(count / self.t_const)\n",
    "            mix_ratio = 1 - ((count % self.t_const) / self.t_const)\n",
    "            inv_mix_ratio = 1 - mix_ratio\n",
    "\n",
    "            prev_image = self.transition_style_seq_list[curr_style_img_index]\n",
    "            next_image = self.transition_style_seq_list[curr_style_img_index + 1]\n",
    "            \n",
    "            prev_is_content_img = False\n",
    "            next_is_content_img = False\n",
    "            if prev_image is None:\n",
    "                prev_image = content_img\n",
    "                prev_is_content_img = True\n",
    "            if next_image is None:\n",
    "                next_image = content_img\n",
    "                next_is_content_img = True\n",
    "            # If both, don't need to apply style transfer\n",
    "            if prev_is_content_img and next_is_content_img:\n",
    "                temp_ghost_frame = cv2.cvtColor(ghost_frame, cv2.COLOR_RGB2BGR) * self.MAX_CHANNEL_INTENSITY\n",
    "                cv2.imwrite(self.config.POST_VID_FRAME_PATH.format(count), temp_ghost_frame)\n",
    "                continue\n",
    "            \n",
    "            if count > 0:\n",
    "                content_img = ((1 - self.config.GHOST_FRAME_TRANSPARENCY) * content_img) + (self.config.GHOST_FRAME_TRANSPARENCY * ghost_frame)\n",
    "            content_img = tf.cast(tf.convert_to_tensor(content_img), tf.float32)\n",
    "\n",
    "            if prev_is_content_img:\n",
    "                blended_img = next_image\n",
    "            elif next_is_content_img:\n",
    "                blended_img = prev_image\n",
    "            else:\n",
    "                prev_style = mix_ratio * prev_image\n",
    "                next_style = inv_mix_ratio * next_image\n",
    "                blended_img = prev_style + next_style\n",
    "\n",
    "            blended_img = tf.cast(tf.convert_to_tensor(blended_img), tf.float32)\n",
    "            expanded_blended_img = tf.constant(tf.expand_dims(blended_img, axis=0))\n",
    "            expanded_content_img = tf.constant(tf.expand_dims(content_img, axis=0))\n",
    "            # Apply style transfer\n",
    "            stylized_img = self.hub_module(expanded_content_img, expanded_blended_img).pop()\n",
    "            stylized_img = tf.squeeze(stylized_img)\n",
    "\n",
    "            # Re-blend\n",
    "            if prev_is_content_img:\n",
    "                prev_style = mix_ratio * content_img\n",
    "                next_style = inv_mix_ratio * stylized_img\n",
    "            if next_is_content_img:\n",
    "                prev_style = mix_ratio * stylized_img\n",
    "                next_style = inv_mix_ratio * content_img\n",
    "            if prev_is_content_img or next_is_content_img:\n",
    "                stylized_img = self.trim_img(prev_style) + self.trim_img(next_style)\n",
    "\n",
    "            if self.config.KEEP_COLORS:\n",
    "                stylized_img = self._color_correct_to_input(content_img, stylized_img)\n",
    "            \n",
    "            ghost_frame = np.asarray(self.trim_img(stylized_img))\n",
    "\n",
    "            temp_ghost_frame = cv2.cvtColor(ghost_frame, cv2.COLOR_RGB2BGR) * self.MAX_CHANNEL_INTENSITY\n",
    "            cv2.imwrite(self.config.POST_VID_FRAME_PATH.format(count), temp_ghost_frame)\n",
    "        self.post_frame_dir = glob.glob(f'{self.config.POST_VID_FRAME_DIR}/*')\n",
    "\n",
    "    def _color_correct_to_input(self, content, generated):\n",
    "        # image manipulations for compatibility with opencv\n",
    "        content = np.array((content * self.MAX_CHANNEL_INTENSITY), dtype=np.float32)\n",
    "        content = cv2.cvtColor(content, cv2.COLOR_BGR2YCR_CB)\n",
    "        generated = np.array((generated * self.MAX_CHANNEL_INTENSITY), dtype=np.float32)\n",
    "        generated = cv2.cvtColor(generated, cv2.COLOR_BGR2YCR_CB)\n",
    "        generated = self.trim_img(generated)\n",
    "        # extract channels, merge intensity and color spaces\n",
    "        color_corrected = np.zeros(generated.shape, dtype=np.float32)\n",
    "        color_corrected[:, :, 0] = generated[:, :, 0]\n",
    "        color_corrected[:, :, 1] = content[:, :, 1]\n",
    "        color_corrected[:, :, 2] = content[:, :, 2]\n",
    "        return cv2.cvtColor(color_corrected, cv2.COLOR_YCrCb2BGR) / self.MAX_CHANNEL_INTENSITY\n",
    "\n",
    "\n",
    "    def generate_stylized_video(self):\n",
    "        self.post_frame_dir = glob.glob(f'{self.config.POST_VID_FRAME_DIR}/*')\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "        print(\"frame width\", self.frame_width)\n",
    "        video_writer = cv2.VideoWriter(self.config.POST_VID_PATH, fourcc, self.config.FPS, (self.frame_width, self.config.FRAME_HEIGHT))\n",
    "\n",
    "        for count, filename in enumerate(sorted(self.post_frame_dir)):\n",
    "            if count % 10 == 0:\n",
    "                print(f\"Saving frame: {(count/len(self.post_frame_dir)):.0%}\")\n",
    "            image = cv2.imread(filename)\n",
    "            video_writer.write(image)\n",
    "\n",
    "        video_writer.release()\n",
    "        print(f\"Style transfer complete! Output at {self.config.POST_VID_PATH}\")\n",
    "    \n",
    "    def run(self):\n",
    "        print(\"Fetching input frames\")\n",
    "        self.fetch_pre_frames()\n",
    "        print(\"Fetching style reference info\")\n",
    "        self.fetch_style_refs()\n",
    "        print(\"Fetching output frames\")\n",
    "        self.fetch_post_frames()\n",
    "        print(\"Saving video\")\n",
    "        self.generate_stylized_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching input frames\n",
      "Fetching style reference info\n",
      "Warning: Resizing style images -> may cause distortion\n",
      "Fetching output frames\n",
      "Output frame: 0%\n",
      "Output frame: 6%\n",
      "Output frame: 12%\n",
      "Output frame: 18%\n",
      "Output frame: 25%\n",
      "Output frame: 31%\n",
      "Output frame: 37%\n",
      "Output frame: 43%\n",
      "Output frame: 49%\n",
      "Output frame: 55%\n",
      "Output frame: 61%\n",
      "Output frame: 67%\n",
      "Output frame: 74%\n",
      "Output frame: 80%\n",
      "Output frame: 86%\n",
      "Output frame: 92%\n",
      "Output frame: 98%\n",
      "Saving video\n",
      "frame width 202\n",
      "Saving frame: 0%\n",
      "Saving frame: 6%\n",
      "Saving frame: 12%\n",
      "Saving frame: 18%\n",
      "Saving frame: 25%\n",
      "Saving frame: 31%\n",
      "Saving frame: 37%\n",
      "Saving frame: 43%\n",
      "Saving frame: 49%\n",
      "Saving frame: 55%\n",
      "Saving frame: 61%\n",
      "Saving frame: 67%\n",
      "Saving frame: 74%\n",
      "Saving frame: 80%\n",
      "Saving frame: 86%\n",
      "Saving frame: 92%\n",
      "Saving frame: 98%\n",
      "Style transfer complete! Output at ./post_video.mp4\n"
     ]
    }
   ],
   "source": [
    "## Run as a script\n",
    "StyleTransfer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./post_frames/frame_0000_.png\n",
      "Saving frame: 0%\n",
      "./post_frames/frame_0001_.png\n",
      "./post_frames/frame_0002_.png\n",
      "./post_frames/frame_0003_.png\n",
      "./post_frames/frame_0004_.png\n",
      "./post_frames/frame_0005_.png\n",
      "./post_frames/frame_0006_.png\n",
      "./post_frames/frame_0007_.png\n",
      "./post_frames/frame_0008_.png\n",
      "./post_frames/frame_0009_.png\n",
      "./post_frames/frame_0010_.png\n",
      "Saving frame: 6%\n",
      "./post_frames/frame_0011_.png\n",
      "./post_frames/frame_0012_.png\n",
      "./post_frames/frame_0013_.png\n",
      "./post_frames/frame_0014_.png\n",
      "./post_frames/frame_0015_.png\n",
      "./post_frames/frame_0016_.png\n",
      "./post_frames/frame_0017_.png\n",
      "./post_frames/frame_0018_.png\n",
      "./post_frames/frame_0019_.png\n",
      "./post_frames/frame_0020_.png\n",
      "Saving frame: 12%\n",
      "./post_frames/frame_0021_.png\n",
      "./post_frames/frame_0022_.png\n",
      "./post_frames/frame_0023_.png\n",
      "./post_frames/frame_0024_.png\n",
      "./post_frames/frame_0025_.png\n",
      "./post_frames/frame_0026_.png\n",
      "./post_frames/frame_0027_.png\n",
      "./post_frames/frame_0028_.png\n",
      "./post_frames/frame_0029_.png\n",
      "./post_frames/frame_0030_.png\n",
      "Saving frame: 18%\n",
      "./post_frames/frame_0031_.png\n",
      "./post_frames/frame_0032_.png\n",
      "./post_frames/frame_0033_.png\n",
      "./post_frames/frame_0034_.png\n",
      "./post_frames/frame_0035_.png\n",
      "./post_frames/frame_0036_.png\n",
      "./post_frames/frame_0037_.png\n",
      "./post_frames/frame_0038_.png\n",
      "./post_frames/frame_0039_.png\n",
      "./post_frames/frame_0040_.png\n",
      "Saving frame: 25%\n",
      "./post_frames/frame_0041_.png\n",
      "./post_frames/frame_0042_.png\n",
      "./post_frames/frame_0043_.png\n",
      "./post_frames/frame_0044_.png\n",
      "./post_frames/frame_0045_.png\n",
      "./post_frames/frame_0046_.png\n",
      "./post_frames/frame_0047_.png\n",
      "./post_frames/frame_0048_.png\n",
      "./post_frames/frame_0049_.png\n",
      "./post_frames/frame_0050_.png\n",
      "Saving frame: 31%\n",
      "./post_frames/frame_0051_.png\n",
      "./post_frames/frame_0052_.png\n",
      "./post_frames/frame_0053_.png\n",
      "./post_frames/frame_0054_.png\n",
      "./post_frames/frame_0055_.png\n",
      "./post_frames/frame_0056_.png\n",
      "./post_frames/frame_0057_.png\n",
      "./post_frames/frame_0058_.png\n",
      "./post_frames/frame_0059_.png\n",
      "./post_frames/frame_0060_.png\n",
      "Saving frame: 37%\n",
      "./post_frames/frame_0061_.png\n",
      "./post_frames/frame_0062_.png\n",
      "./post_frames/frame_0063_.png\n",
      "./post_frames/frame_0064_.png\n",
      "./post_frames/frame_0065_.png\n",
      "./post_frames/frame_0066_.png\n",
      "./post_frames/frame_0067_.png\n",
      "./post_frames/frame_0068_.png\n",
      "./post_frames/frame_0069_.png\n",
      "./post_frames/frame_0070_.png\n",
      "Saving frame: 43%\n",
      "./post_frames/frame_0071_.png\n",
      "./post_frames/frame_0072_.png\n",
      "./post_frames/frame_0073_.png\n",
      "./post_frames/frame_0074_.png\n",
      "./post_frames/frame_0075_.png\n",
      "./post_frames/frame_0076_.png\n",
      "./post_frames/frame_0077_.png\n",
      "./post_frames/frame_0078_.png\n",
      "./post_frames/frame_0079_.png\n",
      "./post_frames/frame_0080_.png\n",
      "Saving frame: 49%\n",
      "./post_frames/frame_0081_.png\n",
      "./post_frames/frame_0082_.png\n",
      "./post_frames/frame_0083_.png\n",
      "./post_frames/frame_0084_.png\n",
      "./post_frames/frame_0085_.png\n",
      "./post_frames/frame_0086_.png\n",
      "./post_frames/frame_0087_.png\n",
      "./post_frames/frame_0088_.png\n",
      "./post_frames/frame_0089_.png\n",
      "./post_frames/frame_0090_.png\n",
      "Saving frame: 55%\n",
      "./post_frames/frame_0091_.png\n",
      "./post_frames/frame_0092_.png\n",
      "./post_frames/frame_0093_.png\n",
      "./post_frames/frame_0094_.png\n",
      "./post_frames/frame_0095_.png\n",
      "./post_frames/frame_0096_.png\n",
      "./post_frames/frame_0097_.png\n",
      "./post_frames/frame_0098_.png\n",
      "./post_frames/frame_0099_.png\n",
      "./post_frames/frame_0100_.png\n",
      "Saving frame: 61%\n",
      "./post_frames/frame_0101_.png\n",
      "./post_frames/frame_0102_.png\n",
      "./post_frames/frame_0103_.png\n",
      "./post_frames/frame_0104_.png\n",
      "./post_frames/frame_0105_.png\n",
      "./post_frames/frame_0106_.png\n",
      "./post_frames/frame_0107_.png\n",
      "./post_frames/frame_0108_.png\n",
      "./post_frames/frame_0109_.png\n",
      "./post_frames/frame_0110_.png\n",
      "Saving frame: 67%\n",
      "./post_frames/frame_0111_.png\n",
      "./post_frames/frame_0112_.png\n",
      "./post_frames/frame_0113_.png\n",
      "./post_frames/frame_0114_.png\n",
      "./post_frames/frame_0115_.png\n",
      "./post_frames/frame_0116_.png\n",
      "./post_frames/frame_0117_.png\n",
      "./post_frames/frame_0118_.png\n",
      "./post_frames/frame_0119_.png\n",
      "./post_frames/frame_0120_.png\n",
      "Saving frame: 74%\n",
      "./post_frames/frame_0121_.png\n",
      "./post_frames/frame_0122_.png\n",
      "./post_frames/frame_0123_.png\n",
      "./post_frames/frame_0124_.png\n",
      "./post_frames/frame_0125_.png\n",
      "./post_frames/frame_0126_.png\n",
      "./post_frames/frame_0127_.png\n",
      "./post_frames/frame_0128_.png\n",
      "./post_frames/frame_0129_.png\n",
      "./post_frames/frame_0130_.png\n",
      "Saving frame: 80%\n",
      "./post_frames/frame_0131_.png\n",
      "./post_frames/frame_0132_.png\n",
      "./post_frames/frame_0133_.png\n",
      "./post_frames/frame_0134_.png\n",
      "./post_frames/frame_0135_.png\n",
      "./post_frames/frame_0136_.png\n",
      "./post_frames/frame_0137_.png\n",
      "./post_frames/frame_0138_.png\n",
      "./post_frames/frame_0139_.png\n",
      "./post_frames/frame_0140_.png\n",
      "Saving frame: 86%\n",
      "./post_frames/frame_0141_.png\n",
      "./post_frames/frame_0142_.png\n",
      "./post_frames/frame_0143_.png\n",
      "./post_frames/frame_0144_.png\n",
      "./post_frames/frame_0145_.png\n",
      "./post_frames/frame_0146_.png\n",
      "./post_frames/frame_0147_.png\n",
      "./post_frames/frame_0148_.png\n",
      "./post_frames/frame_0149_.png\n",
      "./post_frames/frame_0150_.png\n",
      "Saving frame: 92%\n",
      "./post_frames/frame_0151_.png\n",
      "./post_frames/frame_0152_.png\n",
      "./post_frames/frame_0153_.png\n",
      "./post_frames/frame_0154_.png\n",
      "./post_frames/frame_0155_.png\n",
      "./post_frames/frame_0156_.png\n",
      "./post_frames/frame_0157_.png\n",
      "./post_frames/frame_0158_.png\n",
      "./post_frames/frame_0159_.png\n",
      "./post_frames/frame_0160_.png\n",
      "Saving frame: 98%\n",
      "./post_frames/frame_0161_.png\n",
      "./post_frames/frame_0162_.png\n",
      "Style transfer complete! Output at ./post_video.mp4\n"
     ]
    }
   ],
   "source": [
    "# Generate video from post_frames\n",
    "POST_VID_FRAME_DIR = './post_frames'\n",
    "POST_VID_FRAME_FILE = 'frame_{:0>4d}_.png'\n",
    "POST_VID_FRAME_PATH = f'{POST_VID_FRAME_DIR}/{POST_VID_FRAME_FILE}'\n",
    "POST_VID_PATH = './post_video.mp4'\n",
    "FPS = 20\n",
    "FRAME_WIDTH = 202\n",
    "FRAME_HEIGHT = 360\n",
    "\n",
    "\n",
    "post_frame_dir = glob.glob(f'{POST_VID_FRAME_DIR}/*')\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "video_writer = cv2.VideoWriter(POST_VID_PATH, fourcc, FPS, (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "\n",
    "for count, filename in enumerate(sorted(post_frame_dir)):\n",
    "    print(filename)\n",
    "    if count % 10 == 0:\n",
    "        print(f\"Saving frame: {(count/len(post_frame_dir)):.0%}\")\n",
    "    image = cv2.imread(filename)\n",
    "    video_writer.write(image)\n",
    "\n",
    "video_writer.release()\n",
    "print(f\"Style transfer complete! Output at {POST_VID_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
